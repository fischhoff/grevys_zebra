---
title: "grevyâ€™s_zebra"
author: "Ilya"
date: "12/15/2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Mapping preferred habitat of Grevy's zebra

##Objective: Map habitat quality for Grevy's zebra based on radiocollar and remote sensing data. 

##Background
Grevy's zebra are critically endangered. Grevy's zebra depend on the rangelands of Northern Kenya. To prioritize conservation interventions in this area, it is important to identify the environmental features preferred by Grevy's zebra. We expect land cover, woody biomass, and livestock density to be important influences on the locations chosen by Grevy's zebra. Land cover affects forage availability for Grevy's zebra, which are primarily grazers while also infrequently browsing. Woody biomass, in addition to affecting forage, is a proxy for visibility, which drives predation danger. Livestock represent competition and human activity.  

##Strategy
Assemble and integrate Grevy's zebra GPS radiocollar and environmental data. Generate "false" Grevy's zebra locations by drawing random moves from observed distributions of step lengths and turning angles. Use machine learning (generalized boosted models) to identify environmental features that predict whether a Grevy's zebra location is true (observed locations) or false. Use model to map habitat suitability based on environmental features.       

##Data sources
###Grevy's zebra GPS collar locations
###Environmental data
####Global land cover dataset (European Space Agency)
####Woody biomass dataset (Bouvet et al.)
####Livestock density (Food and Agriculture Organization)

##Study design

####install packages needed for study
```{r}
# pkgTest is a helper function to load packages and install packages only when they are not installed yet.
pkgTest <- function(x)
{
  if (x %in% rownames(installed.packages()) == FALSE) {
    install.packages(x, dependencies= TRUE)    
  }
  library(x, character.only = TRUE)
}
neededPackages <- c("raster", "rgdal", "dplyr", "lubridate", "ggplot2",  "geojsonio", 
                    "tmaptools", "geojson", "stplanr", "sf", "tidyverse", "amt", "tibble", "survival", "sp", "lubridate" ,
                    "MuMIn", "lme4", "gamm4", "lmerTest", "ggplot2", "onehot", "scales")

for (package in neededPackages){pkgTest(package)}


```

###Read in and merge Grevy's zebra GPS radiocollar data.

###GZT 2006-2008
####read in collar data 2006-2007 from GZT
```{r}
#need path name to match data organization  
path <- "GZ collar data/GZ-Collar-Data-2006-2007"
#get list of file names
fns <- list.files(path)
#find the files that have .shp extension
shps <- fns[grepl(".shp", fns)]
#find the locations files (vs. tracks)
shps <- shps[grepl("locations", shps)]
#remove the xml files
shps.xml <- shps[grepl(".xml", shps)]
shps <- setdiff(shps, shps.xml)

#check merge locations file to see what's in it
m <- shapefile(paste(path, "/", "merge_locations.shp", sep = ""))
m.df = data.frame(m)
unique(m$COLLAR_ID)

#get list of shpfiles w/o merge file
shps.original = setdiff(shps, "merge_locations.shp")
#get names of zebras 
id.names <- sapply(strsplit(shps.original, "_"), `[`, 1)
#get chronofile name of ids
chrono.names <- sapply(strsplit(shps.original, "_"), `[`, 2)
out = NULL

for (a in 1:length(shps.original)){
  print(a)
  #read in shapefile
  tmp <- shapefile(paste(path, "/", shps.original[a], sep = ""))
  #make into dataframe from shapefile
  tmp<- data.frame(tmp)
  #make names lower case
  names(tmp)<- tolower(names(tmp))
  #assign zebra name
  tmp$id <- id.names[a]
  #assign chronofile.chk (to be checked against chronofile field)
  tmp$chronofile.chk <-chrono.names[a]
  #add records to out
  out = rbind(out, tmp)
}
#confirm that data read into "out"" have same number of rows as in merge file
print(dim(out)[1])==print(dim(m.df)[1])#

#confirm chronofile matches chronofile.chk
print(unique(out$chronofile==out$chronofile.chk))
m0607 <- out
save(m0607,file="m0607.Rdata") 
```

####get summary (metadata) of 06-07 data 
```{r}
load("m0607.Rdata")

m0607$date = as.Date(m0607$fixtime, "%Y/%m/%d")
m0607sum <- m0607 %>% 
  group_by(id) %>%
    summarise(
    first.date.GZT = min(date),
    last.date.GZT = max(date),
    count.GZT = n(),
    collar_id = collar_id[1],
    chronofile = chronofile[1])
print(m0607sum)
m0607sum = data.frame(m0607sum)
save(m0607sum, file = "m0607sum.Rdata") 
```

####Integrate three sources of metadata for 2006-2008 and add info to location data.
GZT 2006-2007 data have date but not time of day. 
Read in collar data 2006-2007 from MBB thesis files; these actually go into 2008. 
Merge metadata extracted from GZT version (m0607sum), with metadata extracted from MBB thesis files (m0606MBBsum), and metadata from Henrik's report (H). 
Add metadata from Henrik report to location data (m0608MBB)
```{r}
load("m0607sum.Rdata")
path <- "Michael/MBB_Thesis/MBB_SCRIPTS/Zebra_csv"

m0608MBB <- read.csv(paste(path, "/", "master.csv", sep = ""))

m0608MBB$name = m0608MBB$id
m0608MBB$Fixtime = as.character(m0608MBB$Fixtime)
m0608MBB$Fixtime <- strptime(m0608MBB$Fixtime, format = "%m/%d/%Y %H:%M", tz = "Africa/Nairobi")
#now get strftime which can be used to get min
m0608MBB$Fixtime <-strftime(m0608MBB$Fixtime)

#MBB data on Silurian are missing from "master", so read those in separately
silurian = "silurian.txt"
sil = read.csv(silurian)
sil$id = "Silurian"
sil$name = "Silurian"
sil$Fixtime <- strptime(sil$Fixtime, format = "%m/%d/%Y %H:%M", tz = "Africa/Nairobi")
#now get strftime which can be used to get min
sil$Fixtime <-strftime(sil$Fixtime)

int.col = intersect(names(sil), names(m0608MBB))
sil = sil[,int.col]
setdiff(names(m0608MBB), names(sil))
setdiff(names(sil), names(m0608MBB))
m0608MBB = rbind(m0608MBB, sil)

#MBB data on jeff are missing from "master", so read those in separately
jeff = "jeff.csv"
jeff = read.csv(jeff)
jeff$id = "Jeff"
jeff$name = "Jeff"
jeff$Fixtime <- strptime(jeff$Fixtime, format = "%m/%d/%Y %H:%M", tz = "Africa/Nairobi")
#now get strftime which can be used to get min
jeff$Fixtime <-strftime(jeff$Fixtime)
int.col = intersect(names(jeff), names(m0608MBB))
jeff = jeff[,int.col]
setdiff(names(m0608MBB), names(jeff))
setdiff(names(jeff), names(m0608MBB))
m0608MBB = rbind(m0608MBB, jeff)

m0608MBBsum <- m0608MBB %>% 
  group_by(id) %>%
    summarise(
    collar_id = collar_id[1],
    chronofile = Chronofile[1],
      first.date.MBB = min(Fixtime),
    last.date.MBB = max(Fixtime),
    count.MBB = n()
    )
m0608MBBsum = data.frame(m0608MBBsum)
m0608MBBsum = m0608MBBsum[order(m0608MBBsum$collar_id),]
print(m0608MBBsum)#seems to be missing Silurian

#put together m0607sum and m0608MBBsum
intersect(names(m0607sum), names(m0608MBBsum))
setdiff(m0607sum$id, m0608MBBsum$id)
setdiff( m0608MBBsum$id, m0607sum$id)
m0607sum$id[m0607sum$id=="njeri"]="Njeri"
#assume that Silurian = Silurian1
m0608MBBsum$id = as.character(m0608MBBsum$id)
m0608MBBsum$id[m0608MBBsum$id=="Silurian"]="Silurian1"
m0608GZT_MBB = merge(m0607sum, m0608MBBsum)
m0608GZT_MBB$first.data.match = m0608GZT_MBB$first.date.GZT==m0608GZT_MBB$first.date.MBB#all except belind match, and that is only two days off
m0608GZT_MBB$count.MBB>m0608GZT_MBB$count.GZT#check that there are always more data in MBB files -- all TRUE

#now read in metadata from Henrik
H = read.csv("metadata_STE_06_07_Henrik.csv", blank.lines.skip = TRUE)
H = subset(H, GZ.ID.final!="")
names(H)[names(H)=="Serial.No."]="collar_id"
names(H)[names(H)=="GZ.ID.final"]="id"
H$id = as.character(H$id)
H$id[H$id == "Silurian"]="Silurian1"
setdiff(H$id, m0608GZT_MBB$id)#Laisamis2 -- active, not reporting
setdiff(m0608GZT_MBB$id, H$id)

Martha = subset(m0608GZT_MBB,id=="Martha")#makes sense that Martha would be missing from Henrik because it was put on after Henrik's report (made in March '07)

Samburu2 = subset(m0608GZT_MBB,id=="Samburu2")#Samburu2 also put on after Henrik's report 
#take certain fields from Henrik
keep.col= c("id", "collar_id","place", "Area.Name", "Sex", "Age..yrs.")
H = H[,keep.col]
#add rows in Henrik dataset for Martha and Samburu2
tmp = data.frame(id = "Martha",
                 collar_id = "T5H-1529",
                 place = "unknown",
                 Area.Name = "unknown",
              Sex = "F",
              Age..yrs. = "unknown")
H = rbind(H, tmp)
tmp = data.frame(id = "Samburu2",
                 collar_id = "T5H-1429",
                 place = "unknown",
                 Area.Name = "unknown",
              Sex = "unknown",
              Age..yrs. = "unknown")
H = rbind(H, tmp)

#merge H with m0608GZT_MBB
m0608GZT_MBB_H = merge(m0608GZT_MBB, H)
#save metadata file
keep.col = c("id", "collar_id", "chronofile", "place", "Area.Name", "Sex", "Age..yrs.")
m0608GZT_MBB_H = m0608GZT_MBB_H[,keep.col]

intersect(names(m0608MBB), names(m0608GZT_MBB_H))
#change Silurian to Silurian1 in m0608MBB
m0608MBB$id = as.character(m0608MBB$id)
m0608MBB$id[m0608MBB$id == "Silurian"]="Silurian1"
m0608_locs = merge(m0608MBB, H)
dim(m0608MBB)[1]==dim(m0608_locs)[1]#should be TRUE and it is 
setdiff(unique(m0608MBB$id), unique(m0608_locs$id))#should be empty
setdiff(unique(m0608GZT_MBB_H$id), unique(m0608_locs$id))#should be empty

save(m0608_locs, file = "m0608_locs.Rdata")
write.csv(m0608_locs, file = "GZ_2006_2008.csv")
```

###2011-2014 data
####Read in collar data from 2011 to 2013 (output: z11_13)
```{r}
path <- "GZ collar data/"
z11_13 = read.csv(file =paste(path,"Collar data from Jan 2011_Dec 2013_PL.csv", sep =""))
z11_13$datetime = strptime(z11_13$acquisitiontime, format = "%d/%m/%Y %H:%M:%S", tz = "Africa/Nairobi")
#now get strftime which can be used to get min
z11_13$datetime <-strftime(z11_13$datetime)
save(z11_13, file = "z11_13.Rdata")
#z11_13$collar_id = z11_13$collarid
z11_13sum <- z11_13 %>%
  group_by(collarid) %>%
      summarise(
    first.date = min(datetime),
    last.date = max(datetime),
    count = n())

#get list of file names, for shapefiles that may overlap with z11_13
fns <- list.files(path)
#find the files that have .shp extension
shps <- fns[grepl(".shp", fns)]
#find the files w/ "11Sep2012"
shps <- shps[grepl("11Sep2012", shps)]
out= NULL
for (a in 1:length(shps)){
  #read in shapefile
  tmp <- shapefile(paste(path, "/", shps[a], sep = ""))
  #make into dataframe from shapefile
  tmp<- data.frame(tmp)
  #make names lower case
  names(tmp)<- tolower(names(tmp))
  #assign zebra name
#  tmp$id <- id.names[a]
  #assign chronofile.chk (to be checked against chronofile field)
#  tmp$chronofile.chk <-chrono.names[a]
  #add records to out
  out = rbind(out, tmp)
}
#summarize out
out$datetime <- strptime(out$datetime, format = "%Y-%m-%d %H:%M:%S", tz = "Africa/Nairobi")
#now get strftime which can be used to get min
out$datetime <-strftime(out$datetime)

out1 <-out %>% 
  group_by(collarid) %>%
    summarise(
    first.date.sep = min(datetime),
    last.date.sep = max(datetime),
    count = n())

#merge z11_13sum and out1
z11_13sum$collarid = as.character(z11_13sum$collarid)
intersect(names(z11_13sum), names(out1))
intersect(unique(out1$collarid), unique(z11_13sum$collarid))
z11_13_out1 = merge(z11_13sum, out1, by = "collarid")
head(z11_13_out1)
#confirmed that separate shapefiles (out) are subset of data in z11_13
```

###Combine all GZT data
####Merge '06-'08 and '11-'14 data
Read in metadata for 2010-2014 data, merge with z11_13. 
Output: Z.Rdata
```{r}
#load 0608 data and change field names to match up with names in metadata for '10-'14
load("m0608_locs.Rdata")
names(m0608_locs)=tolower(names(m0608_locs))
names(m0608_locs)[names(m0608_locs)=="area.name"]="location.collared"
names(m0608_locs)[names(m0608_locs)=="fixtime"]="datetime"
m0608_locs$repro.status.at.collaring = NA
keep = c("id", "collar_id", "datetime", "lon", "lat", "height", "temp", "location.collared", "sex", "age..yrs.", "repro.status.at.collaring")
m0608_locs = m0608_locs[,keep]
names(m0608_locs)
#load '11-'13 data
load("z11_13.Rdata")
names(z11_13)[names(z11_13)=="collarid"]="collar_id"
names(z11_13)[names(z11_13)=="latitude"]="lat"
names(z11_13)[names(z11_13)=="longitude"]="lon"
names(z11_13)[names(z11_13)=="altitude"]="height"
names(z11_13)[names(z11_13)=="temperature"]="temp"
z11_13$id = z11_13$collar_id#id will be the same as collar_id
z11_13$age..yrs. = NA
names(z11_13)
keep = c("id", "collar_id", "datetime", "lon", "lat", "height", "temp", "age..yrs.")
z11_13 = z11_13[,keep]
#left off here 20180427 -need to merge z11_13 with zmeta10_14, then merge with other dataset
#read in metadata for '11-'13
path <- "GZ collar data/"
zmeta_10_14 = read.csv(file = paste(path,"GZ Collar Tracking List July 2014.csv", sep = ""))
names(zmeta_10_14)
head(zmeta_10_14)
names(zmeta_10_14) =tolower(names(zmeta_10_14))
names(zmeta_10_14)[names(zmeta_10_14)=="unit.id"]= "collar_id"
names(zmeta_10_14)[names(zmeta_10_14)=="gz.class.collared"]= "repro.status.at.collaring"
unique(zmeta_10_14$gz.class.collared)#all are female
zmeta_10_14$sex = "F"
names(zmeta_10_14)
keep = c("collar_id", "location.collared", "repro.status.at.collaring", "sex")
zmeta_10_14 = zmeta_10_14[,keep]#keep only some columns in metadata
zmeta_10_14$collar_id = tolower(zmeta_10_14$collar_id)#make collar_id lower case
intersect(names(z11_13), names(zmeta_10_14))
z11_13 = merge(z11_13, zmeta_10_14)

setdiff(names(m0608_locs), names(z11_13))#confirm there are no differences in columns
setdiff(names(z11_13),names(m0608_locs))
intersect(unique(z11_13$collar_id), unique(m0608_locs$collar_id))#confirm there are no ids in common

intersect(names(z11_13), names(m0608_locs))#check names in common

Z = rbind(m0608_locs, z11_13)
save(Z, file = "Z.Rdata")
write.csv(Z, file = "GZcollar_06_13.csv")

```

####Summarize collar data
summary: date range, location collared, sample size
```{r}
library(dplyr)
load("Z.Rdata")

Zsum <- Z %>%
  group_by(id) %>%
  summarise(min.datetime = min(datetime),
            max.datetime = max(datetime),
            location.collared = location.collared[1],
            count = n())

Zsum = data.frame(Zsum)
Zsum = Zsum[order(Zsum$max.datetime),]
Zsum
write.csv(Zsum, file = "Z.summary.csv")
```

###Clean GZ data
####Subset by bounding coordinates matching range of GZ in Kenya.
This removes implausible points. 
```{r}
load("Z.Rdata")

df = Z
proj  = "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"
xy <- cbind(df$lon,df$lat)#package sp

df <-SpatialPointsDataFrame(coords = xy, data = df,
                            proj4string = CRS(proj))

m <- raster("GZ_2010_AGB_watermask_byte_LZW.tif")

projection(m)==projection(df)#check this is true
dim(df)
ZCrop <- raster::crop(df, m)
dim(ZCrop)

Z = ZCrop
save(Z, file = "Z.Rdata")
```

###Read in and merge environmental data. 

###Generate false Grevy's zebra locations. 
Apply amt package to zebra data
```{r}
max_speed = 30#executive decision: cannot go faster than 30 km per hour
#following this vignette: https://cran.r-project.org/web/packages/amt/vignettes/p4_SSF.html
load("Z.Rdata")
nrand =1#number of randomly selected moves chosen
Z$datetime =parse_date_time(Z$datetime,  "YmdHMS")

head(Z$datetime)
ids = unique(Z$id)
a = 1#counter
out = NULL
real = NULL
for (a in 1:length(ids)){#start for loop through ids
  print(a)
  z_tmp = subset(Z, id == ids[a])
  z_tmp = z_tmp[!duplicated(z_tmp$datetime),]#remove duplicate datetimes

  #get one row
  z_tmp1 = z_tmp[1,]
  z_tmp <- mk_track(z_tmp, lon, lat, datetime, id = id)
  #this makes a data.frame
  
  #add burst field
  z_tmp<-z_tmp %>%
    mutate(burst_ = 1)
  #change to tibble
  z_tmp = as_tibble(z_tmp)
  #change to bursts
  z_tmp <- steps_by_burst(z_tmp, lonlat=TRUE)
  speed = rep(NA, dim(z_tmp)[1])
  speed= 60*(z_tmp$sl_/as.numeric(z_tmp$t2_-z_tmp$t1_)/1000)
  z_tmp$speed = speed 
  z_tmp = subset(z_tmp, speed<max_speed)
  #now get bursts again
  z_tmp <- z_tmp %>%
    mutate(x_ = x1_,
           y_ = y1_,
           t_ = t1_)
  
  z_tmp= z_tmp[,c("x_","y_", "t_")]
  z_tmp <- mk_track(z_tmp, x_, y_, t_)
  #this makes a data.frame
    #add burst field
  z_tmp<-z_tmp %>%
    mutate(burst_ = 1)

  #change to tibble
  z_tmp = as_tibble(z_tmp)
  #change to bursts
  z_tmp <- steps_by_burst(z_tmp, lonlat=TRUE)

  z_tmp_real = z_tmp
  z_tmp_real$id = as.character(ids[a])
  real = rbind(real, z_tmp_real)
  #get random steps
  z_tmp <- random_steps(z_tmp, n = nrand)
  #add id field
  z_tmp$id = as.character(ids[a])
  out = rbind(out, z_tmp)
}
head(z_tmp)

out=data.frame(out)
out_false = subset(out, case_ == FALSE)
#https://stackoverflow.com/questions/7477003/calculating-new-longitude-latitude-from-old-n-meters
r_earth = 6371000.0 
out_false$new_latitude  = out_false$y1_  + (out_false$y2_ / r_earth) * (180 / pi)

out_false$new_longitude = out_false$x1_ + (out_false$x2_ / r_earth) * (180 / pi) / cos(out_false$y1_ * pi/180);
#check: https://www.nhc.noaa.gov/gccalc.shtml

#TRUE moves already have x2, y2 as new longitude, new latitude
out_true = subset(out, case_ == TRUE)
out_true$new_latitude = out_true$y2_
out_true$new_longitude = out_true$x2_

out = rbind(out_false, out_true)
out$datetime = parse_date_time(out$t1,  "YmdHMS")

#rename new_longitude as location-long to match format in movebank
names(out)[names(out)=="new_longitude"]="location-long"#use location they went to
names(out)[names(out)=="new_latitude"]="location-lat"
names(out)[names(out)=="t2_"]="timestamp"
#change time stamp to include fraction of second -- did this to use movebank environmental data annotation for generic data. but that has max size 100,000 rows. so commenting out as unnecessary and in case it causes problems reading into regular movebank. 
out$timestamp = as.character(out$timestamp)
# out$timestamp=paste(out$timestamp,".001", sep = "")
out$timestamp <- strptime(out$timestamp, "%Y-%m-%d %H:%M:%OS")#for some reason using strptime and then strftime works
out$timestamp <- strftime(out$timestamp, "%Y-%m-%d %H:%M:%OS", usetz=FALSE)
head(out$timestamp)

out = out[,c("case_", 
              "id","location-long",
             "location-lat",
             "timestamp",
             "sl_",
             "ta_",
             "dt_")]
unique(out$id)
out_init = subset(out, id == ids[1])
out_rest = subset(out, id != ids[1])
write.csv(out_init, file = "out_init.csv")
write.csv(out_rest, file = "out_rest.csv")
save(out, file = "out.Rdata")

#work out cutoff based on horses canter top speed = ~30 km/hr
real = subset(out, case_ == "TRUE")
speed = rep(NA, dim(real)[1])
speed = 60*(real$sl_/as.numeric(real$dt)/1000)
real$speed = speed
save(real, file = "real.Rdata")
write.csv(real, file = "Zreal.csv")
h = hist(real$speed, plot = FALSE, breaks = 200)


```

####Land cover
Read in Africa land cover map and subset for Kenya. Source: http://2016africalandcover20m.esrin.esa.int/
```{r}
path = "ESACCI-LC-L4-LC10-Map-20m-P1Y-2016-v1.0/"
esa <- raster(paste0(path,"ESACCI-LC-L4-LC10-Map-20m-P1Y-2016-v1.0.tif"))

#source: http://thematicmapping.org/downloads/world_borders.php
path = "TM_WORLD_BORDERS-0.3/"
countries = shapefile(paste0(path,"TM_WORLD_BORDERS-0.3.shp"))

kenya = subset(countries, NAME=="Kenya")
proj <- projection(kenya)

esa_kenya = crop(esa, kenya)
plot(esa_kenya)
writeRaster(esa_kenya, "esa_kenya.tif", format="GTiff",
            overwrite=TRUE)

```


####Woody biomass
Read in woody biomass dataset (source: Bouvet et al.) and ESA land cover and put them together. Result: r_comb.tif
```{r woody_esa}
m <- raster("GZ_2010_AGB_watermask_byte_LZW.tif")

##Not using this global land cover map because we found one Africa.
# G = raster("Globcover2009_V2.3_Global_/GLOBCOVER_L4_200901_200912_V2.3.tif")

G = raster("esa_kenya.tif")

projection(m)==projection(G)#check this is true

GCrop <- raster::crop(G, m)

dim(GCrop)
#adapted from SO: https://stackoverflow.com/questions/37956422/resample-raster
Nhr_rows <- dim(GCrop)[1] # resolution of high-res raster
Nhr_cols <- dim(GCrop)[2] # resolution of high-res raster
Nlr_rows <- Nhr_rows/10 # resolution of low-res raster
Nlr_cols <- Nhr_cols/10 # resolution of low-res raster
Nratio <- as.integer(Nhr_rows/Nlr_rows) # ratio of high to low resolutions, to nearest integer value for aggregation

# create some rasters
r.hr <- raster(ncols=Nhr_cols, nrows=Nhr_rows)
r.lr <- raster(ncols=Nlr_cols, nrows=Nlr_rows)
#don't think next row is needed, think it is holdover from debugging
# r.hr[] <- sample(1:13, Nhr^2, replace=T)
r.hr<- GCrop

# aggregate high res raster to *almost* the same resolution as the low res one (to nearest integer number of cells)
# each resulting layerN contains the fraction of area within that cell in which value of original raster is N. 
layer1 <- raster::aggregate(r.hr, Nratio, fun=function(x, na.rm=T) {mean(x==1, na.rm=na.rm)})
layer2 <- raster::aggregate(r.hr, Nratio, fun=function(x, na.rm=T) {mean(x==2, na.rm=na.rm)})
layer3 <- raster::aggregate(r.hr, Nratio, fun=function(x, na.rm=T) {mean(x==3, na.rm=na.rm)})
layer4 <- raster::aggregate(r.hr, Nratio, fun=function(x, na.rm=T) {mean(x==4, na.rm=na.rm)})
layer5 <- raster::aggregate(r.hr, Nratio, fun=function(x, na.rm=T) {mean(x==5, na.rm=na.rm)})
layer6 <- raster::aggregate(r.hr, Nratio, fun=function(x, na.rm=T) {mean(x==6, na.rm=na.rm)})
layer7 <- raster::aggregate(r.hr, Nratio, fun=function(x, na.rm=T) {mean(x==7, na.rm=na.rm)})
layer8 <- raster::aggregate(r.hr, Nratio, fun=function(x, na.rm=T) {mean(x==8, na.rm=na.rm)})
layer10 <- raster::aggregate(r.hr, Nratio, fun=function(x, na.rm=T) {mean(x==10, na.rm=na.rm)})

# and resample low res raster to the desired resolution
layer1 <- raster::resample(layer1, r.lr, method = "ngb") # Note you may prefer method = 'bilinear', depending on purpose.
layer2 <- raster::resample(layer2, r.lr, method = "ngb")
layer3 <- raster::resample(layer3, r.lr, method = "ngb")
layer4 <- raster::resample(layer4, r.lr, method = "ngb")
layer5 <- raster::resample(layer5, r.lr, method = "ngb")
layer6 <- raster::resample(layer6, r.lr, method = "ngb")
layer7 <- raster::resample(layer7, r.lr, method = "ngb")
layer8 <- raster::resample(layer8, r.lr, method = "ngb")
layer10 <- raster::resample(layer10, r.lr, method = "bilinear")

layer_all = stack(layer1,layer2,
                  layer3,
                  layer4,
                  layer5,
                  layer6,
                  layer7,
                  layer8,
                  layer10)


#resample m (woody biomass) so  it as at same spatial scale as coarser-scaled layer_all (esa) 

Nhr_rows <- dim(m)[1] # resolution of high-res raster
Nhr_cols <- dim(m)[2] # resolution of high-res raster
Nlr_rows <- dim(layer1)[1] # resolution of low-res raster
Nlr_cols <- dim(layer1)[2] # resolution of low-res raster
# Nratio <- as.integer(Nhr_rows/Nlr_rows) # ratio of high to low resolutions, to nearest integer value for aggregation
Nratio <- Nhr_rows/Nlr_rows # ratio of high to low resolutions, to nearest integer value for aggregation

r.hr <- raster(ncols=Nhr_cols, nrows=Nhr_rows)
r.lr <- raster(ncols=Nlr_cols, nrows=Nlr_rows)
r.hr<- m

# each resulting layerN contains the fraction of area within that cell in which value of original raster is N. 
layerm <- raster::aggregate(r.hr, Nratio, fun=function(x, na.rm=T) {mean(x, na.rm=na.rm)})

layer_all_resample <- raster::resample(layer_all, layerm, method = "bilinear")

# m_res = resample(layerm,layer1, method = "bilinear")

#add the two layers
r_comb = addLayer(layer_all_resample,layerm)

#save combined layers 
# writeRaster(r_comb, "r_comb.grd", format="raster",
#             overwrite = TRUE)

outfile <- writeRaster(r_comb, filename='r_comb.tif', format="GTiff", overwrite=TRUE,options=c("INTERLEAVE=BAND","COMPRESS=LZW"))

```

####Livestock
Read in cattle, sheep, and goat data and combine
```{r livestock}
#source for cattle data: http://www.fao.org/geonetwork/srv/en/metadata.show?id=47949&currTab=simple
r_comb = stack("r_comb.tif")

C = raster("AFCattle1km_AD_2010_GLW2_01_TIF/AF_Cattle1km_AD_2010_v2_1.tif")
#assign projection
projection(C)=projection(r_comb)#check this is true

#crop
CCrop <- raster::crop(C, r_comb)

#resample C (cattle) so  it as at same spatial scale as finer-scaled r_comb
C_res = resample(CCrop,r_comb, method = "ngb")

#add the two layers
r_comb = addLayer(r_comb,C_res)

S = raster("AF_Sheep1km_AD_2010_GLW2_1_TIF/AF_Sheep1km_AD_2010_v2_1.tif")
projection(S)=projection(r_comb)#check this is true

#crop
SCrop <- raster::crop(S, r_comb)

#resample C (cattle) so  it as at same spatial scale as finer-scaled r_comb
S_res = resample(SCrop,r_comb, method = "ngb")

#add the layer
r_comb = addLayer(r_comb,S_res)

#goats
G = raster("AFGoats1km_AD_2010_v2_1_TIF/AF_Goats1km_AD_2010_v2_1.tif")
projection(G)=projection(r_comb)#check this is true

#crop
GCrop <- raster::crop(G, r_comb)

#resample C (cattle) so  it as at same spatial scale as finer-scaled r_comb
G_res = resample(GCrop,r_comb, method = "ngb")

#add the layer
r_comb = addLayer(r_comb,G_res)

outfile <- writeRaster(r_comb, filename='r_comb1.tif', format="GTiff", overwrite=TRUE,options=c("INTERLEAVE=BAND","COMPRESS=LZW"))

```

###Assign environmental data to true and false Grevy's zebra locations. 

load zebra data and environmental data and extract values 
```{r assign_env}
load("out.Rdata")
out$hour = hour(out$timestamp)
out$month = month(out$timestamp)
out$year = year(out$timestamp)

r_comb_in = stack("r_comb1.tif")

out_spdf = SpatialPointsDataFrame(coords = out[,c("location-long", "location-lat")], 
                               data = out)

 # tell R that out coordinates are in the same lat/lon reference system
# as the woody biomass and land cover 
projection(out_spdf) <- projection(r_comb_in)
locs.vals = raster::extract(r_comb_in, out_spdf,
                            df = TRUE)#specify 

locs.vals.df = data.frame(locs.vals)
locs.vals.df$row = seq(1, dim(locs.vals.df)[1])

#read in legend for GlobCover and add field with description in words
# G_legend = read.csv("/Users/fischhoff/outside_analytics/sdm/Globcover2009_V2.3_Global_/Globcover2009_Legend.csv")

G_legend = read.csv("ESACCI-LC_S2_Prototype_ColorLegend.csv")

####################
##changed this to work for ESA
#names(G_legend)[names(G_legend)=="NB_LAB"]="GLOBCOVER_L4_200901_200912_V2.3"
names(locs.vals.df)[names(locs.vals.df)=="r_comb1.1"]="Tree.cover.areas"
names(locs.vals.df)[names(locs.vals.df)=="r_comb1.2"]="Shrubs.cover.areas"
names(locs.vals.df)[names(locs.vals.df)=="r_comb1.3"]="Grassland"
names(locs.vals.df)[names(locs.vals.df)=="r_comb1.4"]="Cropland"
names(locs.vals.df)[names(locs.vals.df)=="r_comb1.5"]="Vegetation aquatic or regularly flooded"
names(locs.vals.df)[names(locs.vals.df)=="r_comb1.6"]="Lichens Mosses / Sparse vegetation"
names(locs.vals.df)[names(locs.vals.df)=="r_comb1.7"]="Bare areas"
names(locs.vals.df)[names(locs.vals.df)=="r_comb1.8"]="Built up areas"
names(locs.vals.df)[names(locs.vals.df)=="r_comb1.9"]="Open water"
names(locs.vals.df)[names(locs.vals.df)=="r_comb1.10"]="woody.biomass"
names(locs.vals.df)[names(locs.vals.df)=="r_comb1.11"]="cattle"
names(locs.vals.df)[names(locs.vals.df)=="r_comb1.12"]="sheep"
names(locs.vals.df)[names(locs.vals.df)=="r_comb1.13"]="goats"
# encoder <- onehot(locs.vals.df.l, stringsAsFactors=FALSE, 
#                   max_levels = length(unique(locs.vals.df.l$GLOBCOVER)))

####################
# locs.vals.df2 <- predict(encoder, locs.vals.df.l)
# locs.vals.df3 = as.data.frame(locs.vals.df2)

out$row = seq(1, dim(out)[1])

out = merge(out, locs.vals.df, by = "row")

out$case.numeric = as.numeric(out$case_)
summary(out)

out = out[,c(1:14,20:24)]#remove columns that are land covers not observed

out_env = out
save(out_env, file = "out_env.Rdata")
```


##Analysis
###Define generalized boosted model to predict true vs. false. 
###Split Grevy's zebra data into training and test sets
###Fit model and determine training and test accuracy. 
###Predict habitat suitability across landscape by applying fitted model to map of environmental features.  
